[
["index.html", "jiebaR 中文分词文档 1 简介", " jiebaR 中文分词文档 更新于 2017-04-23 1 简介 通过 CRAN 安装： install.packages(&quot;jiebaR&quot;) 新建一个分词器 library(jiebaR) 分词器 = worker() 然后就是分词了， segment(&quot;我是一段文本&quot;, 分词器) #&gt; [1] &quot;我&quot; &quot;是&quot; &quot;一段&quot; &quot;文本&quot; 就是这么简单！ "],
["section-1-1.html", "1.1 遇到了问题？", " 1.1 遇到了问题？ 请将能够重复这个问题的对应代码加上对应的数据（文本）的源文件 发送至用户邮件列表 jiebaR@googlegroups.com 或者访问 https://groups.google.com/d/forum/jiebaR 或者在 GitHub 提交 issues。 "],
["section-1-2.html", "1.2 旧版文档", " 1.2 旧版文档 旧版文档 -->"],
["section-2.html", "2 安装 ", " 2 安装 "],
["cran-.html", "2.1 CRAN 版", " 2.1 CRAN 版 通过CRAN安装: install.packages(&quot;jiebaR&quot;) library(&quot;jiebaR&quot;) "],
["section-2-2.html", "2.2 最新版", " 2.2 最新版 通过Github安装，建议使用 gcc &gt;= 4.9 编译，Windows需要安装 Rtools ： library(devtools) install_github(&quot;qinwf/jiebaRD&quot;) install_github(&quot;qinwf/jiebaR&quot;) library(&quot;jiebaR&quot;) -->"],
["section-3.html", "3 分词", " 3 分词 3.0.1 对文本分词 分词器 = worker() segment(&quot;这是一段测试文本！&quot;, 分词器) #&gt; [1] &quot;这是&quot; &quot;一段&quot; &quot;测试&quot; &quot;文本&quot; 3.0.2 分行输出 $bylines 分词器 = worker() 分词器$bylines = TRUE segment(c(&quot;这是第一行文本。&quot;,&quot;这是第二行文本。&quot;), 分词器) #&gt; [[1]] #&gt; [1] &quot;这是&quot; &quot;第一行&quot; &quot;文本&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;这是&quot; &quot;第二行&quot; &quot;文本&quot; 在新建 worker 时设置 bylines 分词器 = worker(bylines = TRUE) segment(c(&quot;这是第一行文本。&quot;,&quot;这是第二行文本。&quot;), 分词器) #&gt; [[1]] #&gt; [1] &quot;这是&quot; &quot;第一行&quot; &quot;文本&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;这是&quot; &quot;第二行&quot; &quot;文本&quot; 下面是不分行输出的结果 分词器 = worker() segment(c(&quot;这是第一行文本。&quot;,&quot;这是第二行文本。&quot;), 分词器) #&gt; [1] &quot;这是&quot; &quot;第一行&quot; &quot;文本&quot; &quot;这是&quot; &quot;第二行&quot; &quot;文本&quot; 3.0.3 保留符号 $symbol 分词器 = worker() 分词器$symbol = TRUE segment(c(&quot;Hi，这是第一行文本。&quot;), 分词器) #&gt; [1] &quot;Hi&quot; &quot;，&quot; &quot;这是&quot; &quot;第一行&quot; &quot;文本&quot; &quot;。&quot; 重设为不保留符号 分词器$symbol = FALSE segment(c(&quot;Hi，这是第一行文本。&quot;), 分词器) #&gt; [1] &quot;Hi&quot; &quot;这是&quot; &quot;第一行&quot; &quot;文本&quot; 在新建 worker 时设置 symbol 分词器 = worker(symbol = TRUE) segment(c(&quot;Hi，这是第一行文本。&quot;), 分词器) #&gt; [1] &quot;Hi&quot; &quot;，&quot; &quot;这是&quot; &quot;第一行&quot; &quot;文本&quot; &quot;。&quot; segment(c(&quot;。，。；las&quot;), 分词器) #&gt; [1] &quot;。&quot; &quot;，&quot; &quot;。&quot; &quot;；&quot; &quot;las&quot; 分词器 = worker(symbol = FALSE) segment(c(&quot;Hi，这是第一行文本。&quot;), 分词器) #&gt; [1] &quot;Hi&quot; &quot;这是&quot; &quot;第一行&quot; &quot;文本&quot; segment(c(&quot;。，。；las&quot;), 分词器) #&gt; [1] &quot;las&quot; 3.0.4 添加新词到已经新建的分词器中 new_user_word() 分词器 = worker() segment(&quot;这是一个新词&quot;, 分词器) #&gt; [1] &quot;这是&quot; &quot;一个&quot; &quot;新词&quot; # 第三个参数 &quot;n&quot; 代表新词的词性标记 new_user_word(分词器, &quot;这是一个新词&quot;, &quot;n&quot;) #&gt; [1] TRUE segment(&quot;这是一个新词&quot;, 分词器) #&gt; [1] &quot;这是一个新词&quot; 3.0.5 添加停止词 worker(stop_word = “…”) !!!! 对于分词，请不要修改默认加载的停止词文本，即 jiebaR::STOPPATH，请使用自定义的停止词路径。 目录下有一个 stop.txt 文件，内容如下 readLines(&quot;stop.txt&quot;) #&gt; [1] &quot;停止&quot; 分词器 = worker(stop_word = &quot;stop.txt&quot;) segment(&quot;这是一个停止词&quot;, 分词器) #&gt; [1] &quot;这是&quot; &quot;一个&quot; &quot;词&quot; 3.0.6 对文件进行分词 - 使用 readLines 和 writeLines 使用 readLines 函数读取对应文本， texts = readLines(&quot;./index.rmd&quot;, encoding=&quot;UTF-8&quot;) 分词器$bylines = TRUE 分词结果 = segment(texts, 分词器) 使用 writeLines 写入文件 合并各行分词结果 =sapply(分词结果, function(x){ paste(x, collapse = &quot; &quot;)}) writeLines(合并各行分词结果, &quot;./某个文件.txt&quot;) file.remove(&quot;./某个文件.txt&quot;) #&gt; [1] TRUE ！！！！！乱码预警 Windows 下 writeLines 保存的文本可能会为乱码， 对应的解决方案为 writeBin + charToRaw writeBin(charToRaw(&quot;对应文本还有一个换行符\\n&quot;), &quot;./某个文件.txt&quot;) 3.0.7 对文件进行分词 - 自动检测路径 当前目录下又一个 index.rmd 的文件 outputfile = segment(&quot;./index.rmd&quot;, 分词器) readLines(outputfile, 5) #&gt; [1] &quot;&quot; &quot;title jiebaR 中文 分词 文档&quot; #&gt; [3] &quot;knit bookdown render book&quot; &quot;date r paste 更新 于 Sys Date&quot; #&gt; [5] &quot;documentclass book&quot; file.remove(outputfile) #&gt; [1] TRUE 指定输出路径 分词器$output = &quot;某个文件&quot; segment(&quot;./index.rmd&quot;, 分词器) #&gt; [1] &quot;某个文件&quot; readLines(&quot;某个文件&quot;, 5) #&gt; [1] &quot;&quot; &quot;title jiebaR 中文 分词 文档&quot; #&gt; [3] &quot;knit bookdown render book&quot; &quot;date r paste 更新 于 Sys Date&quot; #&gt; [5] &quot;documentclass book&quot; file.remove(&quot;某个文件&quot;) #&gt; [1] TRUE 3.0.8 关闭自动检测路径 $write = “NOFILE” 分词器$write = &quot;NOFILE&quot; head(segment(&quot;./index.rmd&quot;, 分词器)) #&gt; [[1]] #&gt; [1] &quot;index&quot; &quot;rmd&quot; "],
["worker-.html", "4 worker() 初始化参数", " 4 worker() 初始化参数 worker() 用于新建分词引擎，可以同时新建多个分词引擎。引擎的类型有： mix（混合模型）, mp（最大概率模型）, hmm（HMM模型）, query（索引模型）, tag（标记模型）, simhash（Simhash 模型）和 keywords（关键词模型），共7种。 默认参数 worker( type = &quot;mix&quot;, dict = DICTPATH, hmm = HMMPATH, user = USERPATH, idf = IDFPATH, stop_word = STOPPATH, write = T, qmax = 20, topn = 5, encoding = &quot;UTF-8&quot;, detect = T, symbol = F, lines = 1e+05, output = NULL, bylines = F) 4.0.1 type mp（最大概率模型）- 基于词典和词频 hmm（HMM模型）- 基于 HMM 模型，可以发现词典中没有的词 mix（混合模型）- 先用 mp 分，mp 分完调用 hmm 再来把剩余的可能成词的单字分出来。 query（索引模型）- mix 基础上，对大于一定长度的词再进行一次切分。 tag（标记模型）- 词性标记，基于词典的 keywords（关键词模型）- tf-idf 抽 关键词 simhash（Simhash 模型） - 在关键词的基础上计算 simhash 4.0.2 dict 系统词典 优先载入的词典，纯文本文件，默认路径为 jiebaR::DICTPATH 包括词、词频、词性标记三列，用空格分开三列。可以输入自定义路径。 readLines(jiebaR::DICTPATH, 5) #&gt; [1] &quot;1号店 3 n&quot; &quot;1號店 3 n&quot; &quot;4S店 3 n&quot; &quot;4s店 3 n&quot; &quot;AA制 3 n&quot; 4.0.3 user 用户词典 用户词典，包括词、词性标记两列。用户词典中的所有词的词频均为系统词典中的最大词频 (默认，可以通过 user_weight 参数修改)。 4.0.4 user_weight 用户词典中的词的词频，默认为 “max”，系统词典中的最大值。 还可以选 “min” 最小值或者 “median” 中位数。 4.0.5 idf IDF词典 IDF 词典，关键词提取使用。 4.0.6 stop_word 关键词用停止词库 关键词提取使用的停止词库。分词时也可以使用，但是分词时使用的对应路径不能为默认的 jiebaR::STOPPATH。 4.0.7 write 写入文件 是否将文件分词结果写入文件，默认为否。只在输入内容为文件路径时，本参数才会被使用。本参数只对分词和词性标注有效。 4.0.8 qmax 最大索引长度 索引模型中，最大可能成词的字符数。 4.0.9 topn 关键词数 提取的关键词数。 4.0.10 encoding 输入文件编码 输入文件的编码，默认为UTF-8。 4.0.11 detect 检测编码 是否检查输入文件的编码，默认检查。 4.0.12 symbol 保留符号 是否保留符号，默认不保留符号。 4.0.13 lines 读取行数 每次读取文件的最大行数，用于控制读取文件的长度。对于大文件，实现分次读取。 4.0.14 output 输出路径 指定输出路径，一个字符串路径。只在输入内容为文件路径时，本参数才会被使用。 4.0.15 bylines 按行输出 文件结果是否按行输出，如果是，则将读入的文件或字符串向量按行逐个进行分词操作。 "],
["section-5.html", "5 标记和关键词", " 5 标记和关键词 5.0.1 标记 一段文本 = &quot;我爱北京天安门&quot; 标记器 = worker(&quot;tag&quot;) 结果 = tagging(一段文本, 标记器) print(结果) #&gt; r v ns ns #&gt; &quot;我&quot; &quot;爱&quot; &quot;北京&quot; &quot;天安门&quot; names(tagging(一段文本, 标记器)) #&gt; [1] &quot;r&quot; &quot;v&quot; &quot;ns&quot; &quot;ns&quot; 对已经分好词的文本进行标记 分词器 = worker() 分词结果 = segment(一段文本, 分词器) 分词结果 #&gt; [1] &quot;我&quot; &quot;爱&quot; &quot;北京&quot; &quot;天安门&quot; vector_tag(分词结果, 标记器) #&gt; r v ns ns #&gt; &quot;我&quot; &quot;爱&quot; &quot;北京&quot; &quot;天安门&quot; 5.0.2 关键词 topn 控制提取数量 提取器 = worker(&quot;keywords&quot;, topn = 1) keywords(&quot;我爱北京天安门&quot;, 提取器) #&gt; 8.9954 #&gt; &quot;天安门&quot; 对已经分好词的文本进行标记 分词器 = worker() 分词结果 = segment(一段文本, 分词器) 分词结果 #&gt; [1] &quot;我&quot; &quot;爱&quot; &quot;北京&quot; &quot;天安门&quot; vector_keywords(分词结果, 提取器) #&gt; 8.9954 #&gt; &quot;天安门&quot; 5.0.3 Simhash 与海明距离 摘要器 = worker(&quot;simhash&quot;, topn=2) simhash(&quot;江州市长江大桥参加了长江大桥的通车仪式&quot;, 摘要器) #&gt; $simhash #&gt; [1] &quot;12882166450308878002&quot; #&gt; #&gt; $keyword #&gt; 22.3853 8.69667 #&gt; &quot;长江大桥&quot; &quot;江州&quot; distance(&quot;hello world!&quot;, &quot;江州市长江大桥参加了长江大桥的通车仪式&quot;, 摘要器) #&gt; $distance #&gt; [1] 23 #&gt; #&gt; $lhs #&gt; 11.7392 11.7392 #&gt; &quot;hello&quot; &quot;world&quot; #&gt; #&gt; $rhs #&gt; 22.3853 8.69667 #&gt; &quot;长江大桥&quot; &quot;江州&quot; vector_simhash(c(&quot;今天&quot;,&quot;天气&quot;,&quot;真的&quot;,&quot;十分&quot;,&quot;不错&quot;,&quot;的&quot;,&quot;感觉&quot;),摘要器) #&gt; $simhash #&gt; [1] &quot;12098690169796312660&quot; #&gt; #&gt; $keyword #&gt; 6.45994 6.18823 #&gt; &quot;天气&quot; &quot;不错&quot; vector_distance(c(&quot;今天&quot;,&quot;天气&quot;,&quot;真的&quot;,&quot;十分&quot;,&quot;不错&quot;,&quot;的&quot;,&quot;感觉&quot;),c(&quot;今天&quot;,&quot;天气&quot;,&quot;真的&quot;,&quot;十分&quot;,&quot;不错&quot;,&quot;的&quot;,&quot;感觉&quot;),摘要器) #&gt; $distance #&gt; [1] 0 #&gt; #&gt; $lhs #&gt; 6.45994 6.18823 #&gt; &quot;天气&quot; &quot;不错&quot; #&gt; #&gt; $rhs #&gt; 6.45994 6.18823 #&gt; &quot;天气&quot; &quot;不错&quot; 5.0.4 tobin 进行 Simhash 数值的二进制转换。 tobin(&quot;12098690169796312660&quot;) #&gt; [1] &quot;1010011111100111001011101001101110011010001110000011111001010100&quot; 5.0.5 词频统计 freq() freq(c(&quot;测试&quot;, &quot;测试&quot;, &quot;文本&quot;)) #&gt; char freq #&gt; 1 文本 1 #&gt; 2 测试 2 5.0.6 生成 IDF 文件 get_idf() 根据多文档词条结果计算 IDF 值。输入一个包含多个文本向量的 list,每一个文本向量代表一个文档，可自定义停止词列表。 临时输出目录 = tempfile() a_big_list = list(c(&quot;测试&quot;,&quot;一下&quot;),c(&quot;测试&quot;)) get_idf(a_big_list, stop = jiebaR::STOPPATH, path = 临时输出目录) readLines(临时输出目录) #&gt; [1] &quot;一下 0.693147180559945&quot; &quot;测试 0&quot; "],
["section-6.html", "6 其他常用包", " 6 其他常用包 6.0.1 cidian 用来转换搜狗细胞词库 Windows 安装 RTools，设置好对应的环境变量 install.packages(&quot;devtools&quot;) install.packages(&quot;stringi&quot;) install.packages(&quot;pbapply&quot;) install.packages(&quot;Rcpp&quot;) install.packages(&quot;RcppProgress&quot;) library(devtools) install_github(&quot;qinwf/cidian&quot;) 使用 decode_scel(scel = &quot;细胞词库路径&quot;,output = &quot;输出文件路径&quot;) 在线版本 https://cidian.shinyapps.io/shiny-cidian/ 暂时不能用, shinyapps.io 上传文件有 bug？ 6.0.2 text2vec Analyzing Texts with the text2vec Package GloVe Word Embeddings Advanced topics 6.0.3 ropencc 繁简转换需要使用单独的词典包，可以使用 OpenCC 工具实现。R 语言的对应接口有 ropencc 。 OpenCC 開放中文轉換 R 语言接口 devtools::install_github(&quot;qinwf/ropencc&quot;) ccst = converter(S2T) ccst[&quot;开放中文转换&quot;] ccts = converter(T2S) ccts[&quot;開放中文轉換&quot;] 6.0.4 wordcloud2 wordcloud2 - R interface to wordcloud for data visualization. wordcloud2(demoFreqC, size = 2, fontFamily = &quot;微软雅黑&quot;, color = &quot;random-light&quot;, backgroundColor = &quot;grey&quot;) wordcloud2 "],
["section-7.html", "7 常见问题", " 7 常见问题 7.0.1 为什么 tm 包里词都连起来了？ 不建议使用 tm 包。 建议使用 text2vec 包，版本大于 v0.4 对于中文有很好的支持。对于输入的文本，建议均使用 enc2utf8 保证输入的中文文本是 UTF-8 编码。 7.0.1.1 问题描述 tm 包对中文支持不好，直接输入中文就会有遇到下面的情况，应该只在 Windows （各种编码问题….）上会有，Linux下没有这个问题。 library(tm) xx&lt;-c( &quot;进入&quot;, &quot;一个&quot;, &quot;平衡&quot;, &quot;时代&quot;, &quot;现在&quot;, &quot;是&quot;, &quot;住宅&quot;, &quot;价格上涨&quot;, &quot;太快&quot;, &quot;政府&quot;, &quot;采用&quot;, &quot;政策&quot;, &quot;方式&quot;, &quot;调控&quot;, &quot;这些&quot;, &quot;资金&quot;, &quot;就&quot;, &quot;有&quot;, &quot;往&quot;, &quot;商业地产&quot;, &quot;走&quot;, &quot;的&quot;, &quot;趋势&quot;, &quot;因为&quot;, &quot;商业地产&quot;, &quot;把&quot;, &quot;自己&quot;, &quot;划分&quot;, &quot;到&quot;, &quot;这&quot;, &quot;一类&quot;, &quot;去&quot;, &quot;从&quot;, &quot;职业&quot;, &quot;来说&quot;, &quot;我&quot;, &quot;可能&quot;, &quot;是&quot;, &quot;设计师&quot;, &quot;医生&quot;, &quot;老师&quot;, &quot;记者&quot;, &quot;那&quot;, &quot;我&quot;, &quot;就&quot;, &quot;做&quot;, &quot;一个&quot;, &quot;好&quot;, &quot;的&quot;, &quot;记者&quot;, &quot;好&quot;, &quot;的&quot;, &quot;医生&quot;, &quot;这是&quot;, &quot;社会&quot;, &quot;上&quot;, &quot;需要&quot;, &quot;的&quot;, &quot;现在&quot;, &quot;这个&quot;, &quot;时代&quot;, &quot;确实&quot;, &quot;是&quot;, &quot;一个&quot;, &quot;特别&quot;, &quot;好&quot;, &quot;的&quot;, &quot;时代&quot;, &quot;也&quot;, &quot;是&quot;) corpus = Corpus(VectorSource(xx)) dtm_psy = TermDocumentMatrix(corpus) tdm = DocumentTermMatrix(corpus,control = list(wordLengths = c(1, Inf))) inspect(tdm) Terms Docs 20 30 奥运会\\n 把\\n 白领 北京市\\n 背道而驰 本来 不\\n 不大 不好 财经 采用 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7.0.2 空行警告 Output file: C:/test.txt 警告信息： In readLines(input.r, n = lines, encoding = encoding) : 读&quot;C:/test.txt&#39;时最后一行....&quot; 这个信息提示是待处理文本的结尾，应该有一个空行。为什么需要空行，可以参考这里： http://www.zhihu.com/question/20018991 http://segmentfault.com/q/1010000000614237 一般来说这个警告对分词结果没有影响，这不是一个规定，而是一种约定。 一些早期的工具（编辑器／解释器／终端……等）会错误地认为：如果一个文件的结尾不是新行（没有回车或换行符）那么它读取到的内容不是正常结束，而是使用 ^Z（即 EOF Mark）结束的。这会导致这些工具不能正常工作。 一些编辑器，比如 Nano，会自动给每一个文档追加一个新行以避免这个问题。自然而然的，如果一个项目里有人使用了类似 Nano 的编辑器来写代码，他贡献的文件理所当然会有一个新行在结尾。那么制定代码规范的人也就理所当然的要求所有的人遵循这个约定。 虽然现在的编辑器大都先进到可以无视（自动处理）这个问题，但是留一个空行始终还是有一个巨大的好处。 "]
]
